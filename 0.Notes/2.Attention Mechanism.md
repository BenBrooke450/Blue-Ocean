


The self-attention mechanism is a core component of transformers, which power modern large language models (LLMs) like GPT. It allows a model to dynamically weigh the importance of different words in a sequence when encoding each word.




In language, the meaning of a word often depends on other words around it. For example, in the sentence:

 - "The bank can guarantee your deposit will be safe."

The word "bank" could mean a financial institution or the side of a river. Its meaning depends on context—that’s what self-attention helps capture.







## Intuition Summary

Each word gets a new vector that is a mix of all other words, with more "attention" given to the most relevant ones.
